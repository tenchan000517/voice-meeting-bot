import ollama
import os
import aiofiles
from datetime import datetime
from typing import List, Dict, Optional
import logging
import asyncio
import json
import httpx

logger = logging.getLogger(__name__)

class SummarizationService:
    """Service for meeting summarization using Ollama"""
    
    def __init__(self):
        self.client = None
        self.host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        self.model = os.getenv('OLLAMA_MODEL', 'gemma2:2b')
        self.timeout = int(os.getenv('OLLAMA_TIMEOUT', 300))
        self.output_dir = os.getenv('OUTPUT_DIR', './output')
        self._ensure_output_dir()
        
        # Summary templates
        self.templates = {
            'meeting_summary': """
ä»¥ä¸‹ã¯ä¼šè­°ã®æ–‡å­—èµ·ã“ã—ã§ã™ã€‚ã“ã®å†…å®¹ã‚’åŸºã«ã€æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ã„è­°äº‹éŒ²ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ä¼šè­°æƒ…å ±ã€‘
- ä¼šè­°ID: {meeting_id}
- å‚åŠ è€…: {participants}
- æ™‚é–“: {duration}åˆ†

ã€æ–‡å­—èµ·ã“ã—ã€‘
{transcript}

ã€å‡ºåŠ›å½¢å¼ã€‘
# ä¼šè­°è­°äº‹éŒ²

## ğŸ“‹ ä¼šè­°æ¦‚è¦
- **æ—¥æ™‚**: {date}
- **æ™‚é–“**: {duration}åˆ†
- **å‚åŠ è€…**: {participant_count}å

## ğŸ“ ä¸»ãªè­°é¡Œãƒ»å†…å®¹
ï¼ˆã“ã“ã«ä¸»è¦ãªè©±ã—åˆã„ã®å†…å®¹ã‚’3-5ã¤ã®ãƒã‚¤ãƒ³ãƒˆã§æ•´ç†ï¼‰

## âœ… æ±ºå®šäº‹é …
ï¼ˆã“ã“ã«ä¼šè­°ã§æ±ºã¾ã£ãŸã“ã¨ã‚’ç®‡æ¡æ›¸ãã§è¨˜è¼‰ï¼‰

## ğŸ“‹ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
ï¼ˆã“ã“ã«ä»Šå¾Œã®ã‚¿ã‚¹ã‚¯ã‚„å®¿é¡ŒãŒã‚ã‚Œã°è¨˜è¼‰ï¼‰

## ğŸ’­ ãã®ä»–ãƒ»ãƒ¡ãƒ¢
ï¼ˆã“ã“ã«è£œè¶³æƒ…å ±ã‚„è¿½åŠ ãƒ¡ãƒ¢ãŒã‚ã‚Œã°è¨˜è¼‰ï¼‰
""",
            
            'key_points': """
ä»¥ä¸‹ã®ä¼šè­°æ–‡å­—èµ·ã“ã—ã‹ã‚‰ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’3-5ã¤æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

{transcript}

å‡ºåŠ›ã¯ä»¥ä¸‹ã®å½¢å¼ã§ï¼š
â€¢ ãƒã‚¤ãƒ³ãƒˆ1
â€¢ ãƒã‚¤ãƒ³ãƒˆ2
â€¢ ãƒã‚¤ãƒ³ãƒˆ3
""",
            
            'action_items': """
ä»¥ä¸‹ã®ä¼šè­°æ–‡å­—èµ·ã“ã—ã‹ã‚‰ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆä»Šå¾Œã®ã‚¿ã‚¹ã‚¯ãƒ»å®¿é¡Œï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

{transcript}

å‡ºåŠ›ã¯ä»¥ä¸‹ã®å½¢å¼ã§ï¼š
â–¡ ã‚¿ã‚¹ã‚¯1 - æ‹…å½“è€…ï¼ˆã‚‚ã—æ˜è¨˜ã•ã‚Œã¦ã„ã‚Œã°ï¼‰
â–¡ ã‚¿ã‚¹ã‚¯2 - æ‹…å½“è€…ï¼ˆã‚‚ã—æ˜è¨˜ã•ã‚Œã¦ã„ã‚Œã°ï¼‰
"""
        }
    
    def _ensure_output_dir(self):
        """Ensure output directory exists"""
        os.makedirs(self.output_dir, exist_ok=True)
    
    async def initialize(self):
        """Initialize Ollama connection"""
        try:
            logger.info(f"Connecting to Ollama at {self.host}")
            
            # Test connection
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.host}/api/tags")
                if response.status_code == 200:
                    models = response.json()
                    logger.info(f"Available Ollama models: {[m['name'] for m in models.get('models', [])]}")
                    
                    # Check if our model is available
                    model_names = [m['name'] for m in models.get('models', [])]
                    if self.model not in model_names and not any(self.model in name for name in model_names):
                        logger.warning(f"Model {self.model} not found. Available models: {model_names}")
                        # Try to pull the model
                        await self._pull_model()
                    else:
                        logger.info(f"Model {self.model} is available")
                else:
                    raise Exception(f"Failed to connect to Ollama: {response.status_code}")
            
            logger.info("Ollama connection established successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize Ollama: {e}")
            raise
    
    async def _pull_model(self):
        """Pull the required model if not available"""
        try:
            logger.info(f"Pulling model {self.model}...")
            
            async with httpx.AsyncClient(timeout=300.0) as client:
                response = await client.post(
                    f"{self.host}/api/pull",
                    json={"name": self.model}
                )
                
                if response.status_code == 200:
                    logger.info(f"Successfully pulled model {self.model}")
                else:
                    logger.error(f"Failed to pull model {self.model}: {response.status_code}")
                    
        except Exception as e:
            logger.error(f"Error pulling model: {e}")
    
    def is_ready(self) -> bool:
        """Check if summarization service is ready"""
        try:
            # Simple sync check - in production, you might want a more robust check
            return True
        except:
            return False
    
    async def create_summary(
        self,
        meeting_id: str,
        transcript: str,
        participants: List[str],
        duration: int,
        summary_type: str = 'full'
    ) -> str:
        """Create meeting summary using Ollama"""
        try:
            logger.info(f"Creating {summary_type} summary for meeting {meeting_id}")
            
            if summary_type == 'full':
                template = self.templates['meeting_summary']
                prompt = template.format(
                    meeting_id=meeting_id,
                    participants=', '.join(participants),
                    duration=duration,
                    transcript=transcript,
                    date=datetime.now().strftime('%Y-%m-%d %H:%M'),
                    participant_count=len(participants)
                )
            elif summary_type == 'key_points':
                template = self.templates['key_points']
                prompt = template.format(transcript=transcript)
            elif summary_type == 'action_items':
                template = self.templates['action_items']
                prompt = template.format(transcript=transcript)
            else:
                raise ValueError(f"Unknown summary type: {summary_type}")
            
            # Generate summary using Ollama
            summary = await self._generate_with_ollama(prompt)
            
            logger.info(f"Summary generated successfully for meeting {meeting_id}")
            return summary
            
        except Exception as e:
            logger.error(f"Failed to create summary: {e}")
            raise
    
    async def _generate_with_ollama(self, prompt: str) -> str:
        """Generate text using Ollama API"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.host}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,  # Lower temperature for more consistent output
                            "top_p": 0.9,
                            "top_k": 40
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get('response', '')
                else:
                    raise Exception(f"Ollama API error: {response.status_code} - {response.text}")
                    
        except httpx.TimeoutException:
            logger.error("Ollama request timed out")
            raise Exception("Request to Ollama timed out")
        except Exception as e:
            logger.error(f"Ollama generation error: {e}")
            raise
    
    async def create_comprehensive_summary(
        self,
        meeting_id: str,
        transcript: str,
        participants: List[str],
        duration: int
    ) -> Dict[str, str]:
        """Create comprehensive summary with multiple sections"""
        try:
            logger.info(f"Creating comprehensive summary for meeting {meeting_id}")
            
            # Generate different types of summaries concurrently
            tasks = []
            
            # Full summary
            tasks.append(
                self.create_summary(meeting_id, transcript, participants, duration, 'full')
            )
            
            # Key points
            tasks.append(
                self.create_summary(meeting_id, transcript, participants, duration, 'key_points')
            )
            
            # Action items
            tasks.append(
                self.create_summary(meeting_id, transcript, participants, duration, 'action_items')
            )
            
            # Wait for all summaries to complete
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            summary_data = {
                'full_summary': results[0] if not isinstance(results[0], Exception) else "è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ",
                'key_points': results[1] if not isinstance(results[1], Exception) else "é‡è¦ãƒã‚¤ãƒ³ãƒˆã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ",
                'action_items': results[2] if not isinstance(results[2], Exception) else "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ",
                'meeting_id': meeting_id,
                'generated_at': datetime.now().isoformat(),
                'participants': participants,
                'duration_minutes': duration
            }
            
            logger.info(f"Comprehensive summary completed for meeting {meeting_id}")
            return summary_data
            
        except Exception as e:
            logger.error(f"Failed to create comprehensive summary: {e}")
            raise
    
    async def save_summary(self, meeting_id: str, summary_data: Dict[str, str]) -> str:
        """Save summary to markdown file"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"meeting_{meeting_id}_{timestamp}.md"
            file_path = os.path.join(self.output_dir, filename)
            
            # Create markdown content
            if isinstance(summary_data, dict):
                markdown_content = self._format_comprehensive_markdown(summary_data)
            else:
                # Simple string summary
                markdown_content = f"# ä¼šè­°è­°äº‹éŒ²\n\n**ä¼šè­°ID**: {meeting_id}\n**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n{summary_data}"
            
            # Save to file
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(markdown_content)
            
            logger.info(f"Summary saved to: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"Failed to save summary: {e}")
            raise
    
    def _format_comprehensive_markdown(self, summary_data: Dict[str, str]) -> str:
        """Format comprehensive summary as markdown"""
        content = f"""# ğŸ™ï¸ ä¼šè­°è­°äº‹éŒ²

**ä¼šè­°ID**: `{summary_data.get('meeting_id', 'N/A')}`  
**ç”Ÿæˆæ—¥æ™‚**: {summary_data.get('generated_at', datetime.now().isoformat())}  
**å‚åŠ è€…**: {', '.join(summary_data.get('participants', []))}  
**éŒ²éŸ³æ™‚é–“**: {summary_data.get('duration_minutes', 0)}åˆ†

---

## ğŸ“‹ è©³ç´°è­°äº‹éŒ²

{summary_data.get('full_summary', 'è©³ç´°è­°äº‹éŒ²ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ')}

---

## â­ é‡è¦ãƒã‚¤ãƒ³ãƒˆ

{summary_data.get('key_points', 'é‡è¦ãƒã‚¤ãƒ³ãƒˆãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ')}

---

## âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 

{summary_data.get('action_items', 'ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')}

---

*ã“ã®è­°äº‹éŒ²ã¯ AI ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã«èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã®ã§ã€å¿…è¦ã«å¿œã˜ã¦ç¢ºèªãƒ»ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚*
"""
        return content
    
    async def create_hierarchical_summary(
        self,
        meeting_id: str,
        chunk_transcripts: List[Dict[str, str]],
        participants: List[str],
        total_duration: int
    ) -> Dict[str, str]:
        """Create hierarchical summary for long meetings"""
        try:
            logger.info(f"Creating hierarchical summary for {len(chunk_transcripts)} chunks")
            
            # Phase 1: Summarize each chunk individually
            chunk_summaries = []
            for i, chunk in enumerate(chunk_transcripts):
                logger.info(f"Summarizing chunk {i+1}/{len(chunk_transcripts)}")
                
                chunk_prompt = f"""
ä»¥ä¸‹ã¯ä¼šè­°ã®ä¸€éƒ¨ï¼ˆ{i*30}åˆ†ã€œ{(i+1)*30}åˆ†ï¼‰ã®æ–‡å­—èµ·ã“ã—ã§ã™ã€‚
ã“ã®éƒ¨åˆ†ã®è¦ç‚¹ã‚’200æ–‡å­—ç¨‹åº¦ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€æ–‡å­—èµ·ã“ã—ã€‘
{chunk['text']}

ã€è¦ç´„ã€‘
"""
                chunk_summary = await self._generate_with_ollama(chunk_prompt)
                chunk_summaries.append({
                    'chunk_index': i,
                    'time_range': f"{i*30}åˆ†ã€œ{(i+1)*30}åˆ†",
                    'summary': chunk_summary.strip()
                })
                
                # Small delay to avoid overwhelming Ollama
                await asyncio.sleep(1)
            
            # Phase 2: Create final summary from chunk summaries
            combined_summaries = "\n\n".join([
                f"ã€{cs['time_range']}ã€‘\n{cs['summary']}" 
                for cs in chunk_summaries
            ])
            
            final_prompt = f"""
ä»¥ä¸‹ã¯{total_duration}åˆ†é–“ã®ä¼šè­°ã®å„30åˆ†ã”ã¨ã®è¦ç´„ã§ã™ã€‚
ã“ã‚Œã‚‰ã‚’çµ±åˆã—ã¦ã€ä¼šè­°å…¨ä½“ã®è­°äº‹éŒ²ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ä¼šè­°æƒ…å ±ã€‘
- ä¼šè­°ID: {meeting_id}
- å‚åŠ è€…: {', '.join(participants)}
- ç·æ™‚é–“: {total_duration}åˆ†

ã€å„æ™‚é–“å¸¯ã®è¦ç´„ã€‘
{combined_summaries}

ã€å‡ºåŠ›å½¢å¼ã€‘
# ä¼šè­°è­°äº‹éŒ²

## ğŸ“‹ ä¼šè­°æ¦‚è¦
- **æ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
- **æ™‚é–“**: {total_duration}åˆ†
- **å‚åŠ è€…**: {len(participants)}å

## ğŸ“ ä¼šè­°ã®æµã‚Œ
ï¼ˆæ™‚ç³»åˆ—ã§ã®ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’è¨˜è¼‰ï¼‰

## â­ ä¸»ãªè­°é¡Œãƒ»æ±ºå®šäº‹é …
ï¼ˆé‡è¦ãªæ±ºå®šäº‹é …ã‚’ç®‡æ¡æ›¸ãã§ï¼‰

## âœ… ä»Šå¾Œã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
ï¼ˆå¿…è¦ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¨˜è¼‰ï¼‰

## ğŸ“Œ è£œè¶³äº‹é …
ï¼ˆãã®ä»–ã®é‡è¦äº‹é …ãŒã‚ã‚Œã°è¨˜è¼‰ï¼‰
"""
            
            final_summary = await self._generate_with_ollama(final_prompt)
            
            # Return comprehensive result
            return {
                'full_summary': final_summary,
                'chunk_summaries': chunk_summaries,
                'meeting_id': meeting_id,
                'generated_at': datetime.now().isoformat(),
                'participants': participants,
                'duration_minutes': total_duration,
                'chunk_count': len(chunk_transcripts)
            }
            
        except Exception as e:
            logger.error(f"Failed to create hierarchical summary: {e}")
            raise
    
    async def cleanup_old_summaries(self, days: int = 30):
        """Clean up old summary files"""
        try:
            current_time = datetime.now()
            cutoff_time = current_time.timestamp() - (days * 24 * 60 * 60)
            
            for filename in os.listdir(self.output_dir):
                if filename.endswith('.md'):
                    file_path = os.path.join(self.output_dir, filename)
                    file_stat = os.stat(file_path)
                    
                    if file_stat.st_mtime < cutoff_time:
                        os.remove(file_path)
                        logger.info(f"Cleaned up old summary: {filename}")
                        
        except Exception as e:
            logger.error(f"Summary cleanup failed: {e}")
    
    def get_available_models(self) -> List[str]:
        """Get list of available Ollama models"""
        try:
            # This would be implemented with a synchronous request
            # For now, return common models
            return [
                'gemma2:2b',
                'gemma2:9b',
                'llama3.1:8b',
                'mistral:7b',
                'codellama:7b'
            ]
        except Exception as e:
            logger.error(f"Failed to get models: {e}")
            return []
    
    async def test_generation(self) -> bool:
        """Test if the service can generate text"""
        try:
            test_prompt = "ã“ã‚Œã¯æ¥ç¶šãƒ†ã‚¹ãƒˆã§ã™ã€‚ã€ŒOKã€ã¨ã ã‘å›ç­”ã—ã¦ãã ã•ã„ã€‚"
            result = await self._generate_with_ollama(test_prompt)
            logger.info(f"Test generation result: {result}")
            return len(result.strip()) > 0
        except Exception as e:
            logger.error(f"Test generation failed: {e}")
            return False